\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2017
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2017}

\usepackage[final]{nips_2017}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2017}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{natbib}

\title{Cracking Neural Network Hashes with Adversarial Examples}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  Conrad ~Christensen\\
  Deep Learning, Spring 2018\\
  New York University\\
  \href{mailto:conradbc@cims.nyu.edu}{\texttt{conradbc@cims.nyu.edu}} \\
  \And
  Len ~Ying\\
  Deep Learning, Spring 2018\\
  New York University\\
  \href{mailto:YOUR-EMAIL@domain.edu}{\texttt{YOUR-EMAIL@domain.edu}}
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
    Success of neural networks have lead to a wide range of applications, some
    more appropriate than others. Due to a potential for parallelization, and
    ease of implementation, some works have suggested using neural networks as
    hash functions.  We believe this is a bad idea in terms of security, as
    neural networks are differentiable, which allows methods for collision
    search. We show that it is possible to generate hash collisions for some of
    the basic neural network hash functions. Additionally we explore the
    applicability of multiple techniques from adversarial example generation on
    generating collisions for a neural network hashing algorithm, comparing
    their efficacy.
\end{abstract}

\section{Literature Review}

Perform a literature review: what work has been done in this area already? How does
your work relate to previous work? 

Here is a simple citation of \cite{DBLP:journals/corr/SzegedyZSBEGF13} here.

\section{Baselines}

Replicate (actually re-code in PyTorch) and run some baselines to which you will
compare your final results.

\section{Initial Idea}

Describe your initial idea: why it is new, interesting or useful. Include a description
of your idea for a new architecture, task, or dataset. (Due for this homework.)
This does not have to be final, you may find that as you collect results from your
experiments, you will want to change your approach based on what works and what
doesn't.

\bibliographystyle{unsrt}
\bibliography{ref} 

\end{document}
